
{
  "commitSha": "7c291d0737d1798109a51b127e86ac3b4464ec5a",
  "filteredNewTestTargets": [
    {
      "testTarget": {
        "functionName": "to_json",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "to_json",
            "lines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "to_json",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ]
          },
          "uncoveredFnBody": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred: # Untested\n            starred = True # Untested\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
          "callGraphToTestedFunction": [
            "to_json"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "to_json",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "to_json",
            "lines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "to_json",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ]
          },
          "uncoveredFnBody": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred: # Untested\n            starred = True # Untested\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
          "callGraphToTestedFunction": [
            "to_json"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "to_json",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "to_json",
            "lines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "to_json",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 132,
                "endLine": 132
              },
              {
                "startLine": 134,
                "endLine": 135
              },
              {
                "startLine": 137,
                "endLine": 137
              },
              {
                "startLine": 139,
                "endLine": 140
              },
              {
                "startLine": 142,
                "endLine": 142
              },
              {
                "startLine": 144,
                "endLine": 144
              }
            ]
          },
          "uncoveredFnBody": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred: # Untested\n            starred = True # Untested\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
          "callGraphToTestedFunction": [
            "to_json"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "check_name_acceptable",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "check_name_acceptable",
            "lines": [
              {
                "startLine": 505,
                "endLine": 506
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_name_acceptable",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 505,
                "endLine": 506
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN: # Untested\n            raise InvalidNameLength() # Untested\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "check_name_acceptable"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "default_preference_list",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_meta.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_meta.py",
            "functionName": "default_preference_list",
            "lines": [
              {
                "startLine": 331,
                "endLine": 333
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_meta.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "default_preference_list",
            "filePath": "pretty_bad_protocol/_meta.py",
            "uncoveredLines": [
              {
                "startLine": 331,
                "endLine": 333
              }
            ]
          },
          "uncoveredFnBody": "class GPGBase:\n    \"\"\"Base class for storing properties and controlling process initialisation.\n\n    :const _result_map: A *dict* containing classes from\n                        :mod:`~gnupg._parsers`, used for parsing results\n                        obtained from GnuPG commands.\n    :const _decode_errors: How to handle encoding errors.\n    \"\"\"\n\n    __metaclass__ = GPGMeta\n    _decode_errors = \"strict\"\n    _result_map = {\n        \"crypt\": _parsers.Crypt,\n        \"delete\": _parsers.DeleteResult,\n        \"generate\": _parsers.GenKey,\n        \"import\": _parsers.ImportResult,\n        \"export\": _parsers.ExportResult,\n        \"list\": _parsers.ListKeys,\n        \"sign\": _parsers.Sign,\n        \"verify\": _parsers.Verify,\n        \"expire\": _parsers.KeyExpirationResult,\n        \"signing\": _parsers.KeySigningResult,\n        \"packets\": _parsers.ListPackets,\n    }\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()\n\n    def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                os.environ = environment\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)\n\n    @property\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs\n\n    @default_preference_list.setter\n    def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs) # Untested\n        if prefs is not None: # Untested\n            self._prefs = prefs # Untested\n\n    @default_preference_list.deleter\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"\n\n    @property\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver\n\n    @keyserver.setter\n    def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location\n\n    @keyserver.deleter\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n\n    def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir\n\n    def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd\n\n    homedir = _util.InheritableProperty(_homedir_getter, _homedir_setter)\n\n    def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys\n\n    def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd\n\n    _generated_keys = _util.InheritableProperty(_generated_keys_getter, _generated_keys_setter)\n\n    def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)\n\n    def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring:\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd\n\n    def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )\n\n    def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` \n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)\n\n    def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))\n\n    def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose\n\n    def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()\n\n    def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result\n\n    def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result\n\n    def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result\n\n    def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result\n\n    def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "callGraphToTestedFunction": [
            "default_preference_list"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "keyserver",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_meta.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_meta.py",
            "functionName": "keyserver",
            "lines": [
              {
                "startLine": 366,
                "endLine": 366
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_meta.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "keyserver",
            "filePath": "pretty_bad_protocol/_meta.py",
            "uncoveredLines": [
              {
                "startLine": 366,
                "endLine": 366
              }
            ]
          },
          "uncoveredFnBody": "class GPGBase:\n    \"\"\"Base class for storing properties and controlling process initialisation.\n\n    :const _result_map: A *dict* containing classes from\n                        :mod:`~gnupg._parsers`, used for parsing results\n                        obtained from GnuPG commands.\n    :const _decode_errors: How to handle encoding errors.\n    \"\"\"\n\n    __metaclass__ = GPGMeta\n    _decode_errors = \"strict\"\n    _result_map = {\n        \"crypt\": _parsers.Crypt,\n        \"delete\": _parsers.DeleteResult,\n        \"generate\": _parsers.GenKey,\n        \"import\": _parsers.ImportResult,\n        \"export\": _parsers.ExportResult,\n        \"list\": _parsers.ListKeys,\n        \"sign\": _parsers.Sign,\n        \"verify\": _parsers.Verify,\n        \"expire\": _parsers.KeyExpirationResult,\n        \"signing\": _parsers.KeySigningResult,\n        \"packets\": _parsers.ListPackets,\n    }\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()\n\n    def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                os.environ = environment\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)\n\n    @property\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs\n\n    @default_preference_list.setter\n    def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs)\n        if prefs is not None:\n            self._prefs = prefs\n\n    @default_preference_list.deleter\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"\n\n    @property\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver\n\n    @keyserver.setter\n    def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location # Untested\n\n    @keyserver.deleter\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n\n    def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir\n\n    def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd\n\n    homedir = _util.InheritableProperty(_homedir_getter, _homedir_setter)\n\n    def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys\n\n    def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd\n\n    _generated_keys = _util.InheritableProperty(_generated_keys_getter, _generated_keys_setter)\n\n    def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)\n\n    def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring:\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd\n\n    def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )\n\n    def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` \n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)\n\n    def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))\n\n    def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose\n\n    def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()\n\n    def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result\n\n    def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result\n\n    def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result\n\n    def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result\n\n    def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "callGraphToTestedFunction": [
            "keyserver"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "summary",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "summary",
            "lines": [
              {
                "startLine": 1380,
                "endLine": 1384
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "summary",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 1380,
                "endLine": 1384
              }
            ]
          },
          "uncoveredFnBody": "class ImportResult:\n    \"\"\"Parse GnuPG status messages for key import operations.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key import operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: A map from GnuPG codes shown with the ``IMPORT_OK`` status message\n        #: to their human-meaningful English equivalents.\n        self._ok_reason = {\n            \"0\": \"Not actually changed\",\n            \"1\": \"Entirely new key\",\n            \"2\": \"New user IDs\",\n            \"4\": \"New signatures\",\n            \"8\": \"New subkeys\",\n            \"16\": \"Contains private key\",\n            \"17\": \"Contains private key\",\n        }\n\n        #: A map from GnuPG codes shown with the ``IMPORT_PROBLEM`` status\n        #: message to their human-meaningful English equivalents.\n        self._problem_reason = {\n            \"0\": \"No specific reason given\",\n            \"1\": \"Invalid Certificate\",\n            \"2\": \"Issuer Certificate missing\",\n            \"3\": \"Certificate Chain too long\",\n            \"4\": \"Error storing certificate\",\n        }\n\n        #: All the possible status messages pertaining to actions taken while\n        #: importing a key.\n        self._fields = \"\"\"count no_user_id imported imported_rsa unchanged\n        n_uids n_subk n_sigs n_revoc sec_read sec_imported sec_dups\n        not_imported\"\"\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: imported.\n        self.fingerprints = list()\n\n        #: A list containing dictionaries with information gathered on keys\n        #: imported.\n        self.results = list()\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have imported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0\n\n    __bool__ = __nonzero__\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        if key == \"IMPORTED\":\n            # this duplicates info we already see in import_ok & import_problem\n            pass\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n        elif key == \"KEY_CONSIDERED\":\n            self.results.append(\n                {\n                    \"status\": key.replace(\"_\", \" \").lower(),\n                }\n            )\n        elif key == \"NODATA\":\n            self.results.append({\"fingerprint\": None, \"status\": \"No valid data found\"})\n        elif key == \"IMPORT_OK\":\n            reason, fingerprint = value.split()\n            reasons = []\n            for code, text in self._ok_reason.items():\n                if int(reason) == int(code):\n                    reasons.append(text)\n            reasontext = \"\\n\".join(reasons) + \"\\n\"\n            self.results.append({\"fingerprint\": fingerprint, \"status\": reasontext})\n            self.fingerprints.append(fingerprint)\n        elif key == \"IMPORT_PROBLEM\":\n            try:\n                reason, fingerprint = value.split()\n            except:  # noqa: E722\n                reason = value\n                fingerprint = \"<unknown>\"\n            self.results.append(\n                {\"fingerprint\": fingerprint, \"status\": self._problem_reason[reason]}\n            )\n        elif key == \"IMPORT_RES\":\n            import_res = value.split()\n            for x in self.counts:\n                self.counts[x] = int(import_res.pop(0))\n        elif key == \"KEYEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Key expired\"}\n            self.results.append(res)\n        # Accoring to docs/DETAILS L859, SIGEXPIRED is obsolete:\n        # \"Removed on 2011-02-04. This is deprecated in favor of KEYEXPIRED.\"\n        elif key == \"SIGEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Signature expired\"}\n            self.results.append(res)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n    def summary(self):  # type: ignore[no-untyped-def]\n        l = []  # noqa: E741 # Untested\n        l.append(\"%d imported\" % self.counts[\"imported\"]) # Untested\n        if self.counts[\"not_imported\"]: # Untested\n            l.append(\"%d not imported\" % self.counts[\"not_imported\"]) # Untested\n        return \", \".join(l) # Untested",
          "callGraphToTestedFunction": [
            "summary"
          ]
        }
      },
      "filteredBy": "duplicate"
    },
    {
      "testTarget": {
        "functionName": "display_name",
        "replayTestFileNames": [],
        "file": "i18n.py",
        "callGraph": [
          {
            "file": "i18n.py",
            "functionName": "display_name",
            "lines": [
              {
                "startLine": 59,
                "endLine": 59
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "i18n.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "display_name",
            "filePath": "i18n.py",
            "uncoveredLines": [
              {
                "startLine": 59,
                "endLine": 59
              }
            ]
          },
          "uncoveredFnBody": "class RequestLocaleInfo:\n    \"\"\"\n    Convenience wrapper around a babel.core.Locale.\n    \"\"\"\n\n    def __init__(self, locale: str):\n        self.locale = Locale.parse(locale)\n\n        # This attribute can be set to `True` to differentiate multiple\n        # locales currently available (supported) for the same language.\n        self.use_display_name = False\n\n    def __str__(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n        \"\"\"\n        return str(self.locale)\n\n    @property\n    def display_name(self) -> str:\n        \"\"\"\n        Give callers (i.e., templates) the `Locale` object's display name when\n        such resolution is warranted, otherwise the language name---as\n        determined by `map_locale_display_names()`.\n        \"\"\"\n        if self.use_display_name:\n            return self.locale.display_name # Untested\n        return self.locale.language_name\n\n    @property\n    def text_direction(self) -> str:\n        \"\"\"\n        The Babel text direction: ltr or rtl.\n\n        Used primarily to set text direction in HTML via the \"dir\"\n        attribute.\n        \"\"\"\n        return self.locale.text_direction\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        The Babel language name.\n\n        Just the language, without subtag info like region or script.\n        \"\"\"\n        return self.locale.language\n\n    @property\n    def id(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n\n        This should match the name of the directory containing its\n        translations.\n        \"\"\"\n        return str(self.locale)\n\n    @property\n    def language_tag(self) -> str:\n        \"\"\"\n        Returns a BCP47/RFC5646 language tag for the locale.\n\n        Language tags are used in HTTP headers and the HTML lang\n        attribute.\n        \"\"\"\n        return get_locale_identifier(parse_locale(str(self.locale)), sep=\"-\")",
          "callGraphToTestedFunction": [
            "display_name"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "configure_babel",
        "replayTestFileNames": [],
        "file": "i18n.py",
        "callGraph": [
          {
            "file": "i18n.py",
            "functionName": "configure_babel",
            "lines": [
              {
                "startLine": 116,
                "endLine": 116
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "i18n.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "configure_babel",
            "filePath": "i18n.py",
            "uncoveredLines": [
              {
                "startLine": 116,
                "endLine": 116
              }
            ]
          },
          "uncoveredFnBody": "def configure_babel(config: SecureDropConfig, app: Flask) -> Babel:\n    \"\"\"\n    Set up Flask-Babel according to the SecureDrop configuration.\n    \"\"\"\n    # Tell Babel where to find our translations.\n    translations_directory = str(config.TRANSLATION_DIRS.absolute())\n    app.config[\"BABEL_TRANSLATION_DIRECTORIES\"] = translations_directory\n\n    # Create the app's Babel instance. Passing the app to the\n    # constructor causes the instance to attach itself to the app.\n    babel = Babel(app)\n\n    # verify that Babel is only using the translations we told it about\n    if list(babel.translation_directories) != [translations_directory]:\n        raise ValueError( # Untested\n            f\"Babel translation directories ({babel.translation_directories}) do not match \"\n            f\"SecureDrop configuration ({[translations_directory]})\"\n        )\n\n    # register the function used to determine the locale of a request\n    babel.localeselector(lambda: get_locale(config))\n    return babel",
          "callGraphToTestedFunction": [
            "configure_babel"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "validate_locale_configuration",
        "replayTestFileNames": [],
        "file": "i18n.py",
        "callGraph": [
          {
            "file": "i18n.py",
            "functionName": "validate_locale_configuration",
            "lines": [
              {
                "startLine": 149,
                "endLine": 149
              },
              {
                "startLine": 155,
                "endLine": 155
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "i18n.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "validate_locale_configuration",
            "filePath": "i18n.py",
            "uncoveredLines": [
              {
                "startLine": 149,
                "endLine": 149
              },
              {
                "startLine": 155,
                "endLine": 155
              }
            ]
          },
          "uncoveredFnBody": "def validate_locale_configuration(config: SecureDropConfig, babel: Babel) -> Set[Locale]:\n    \"\"\"\n    Check that configured locales are available in the filesystem and therefore usable by\n    Babel.  Warn about configured locales that are not usable, unless we're left with\n    no usable default or fallback locale, in which case raise an exception.\n    \"\"\"\n    # These locales are available and loadable from the filesystem.\n    available = set(babel.list_translations())\n    available.add(Locale.parse(FALLBACK_LOCALE))\n\n    # These locales were configured via \"securedrop-admin sdconfig\", meaning\n    # they were present on the Admin Workstation at \"securedrop-admin\" runtime.\n    configured = parse_locale_set(config.SUPPORTED_LOCALES)\n\n    # The intersection of these sets is the set of locales usable by Babel.\n    usable = available & configured\n\n    missing = configured - usable\n    if missing:\n        babel.app.logger.error(\n            f\"Configured locales {missing} are not in the set of usable locales {usable}\"\n        )\n\n    defaults = parse_locale_set([config.DEFAULT_LOCALE, FALLBACK_LOCALE])\n    if not defaults & usable:\n        raise ValueError( # Untested\n            f\"None of the default locales {defaults} are in the set of usable locales {usable}\"\n        )\n\n    return usable",
          "callGraphToTestedFunction": [
            "validate_locale_configuration"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "map_locale_display_names",
        "replayTestFileNames": [],
        "file": "i18n.py",
        "callGraph": [
          {
            "file": "i18n.py",
            "functionName": "map_locale_display_names",
            "lines": [
              {
                "startLine": 185,
                "endLine": 185
              },
              {
                "startLine": 190,
                "endLine": 190
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "i18n.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "map_locale_display_names",
            "filePath": "i18n.py",
            "uncoveredLines": [
              {
                "startLine": 185,
                "endLine": 185
              },
              {
                "startLine": 190,
                "endLine": 190
              }
            ]
          },
          "uncoveredFnBody": "def map_locale_display_names(\n    config: SecureDropConfig, usable_locales: Set[Locale]\n) -> OrderedDict[str, RequestLocaleInfo]:\n    \"\"\"\n    Create a map of locale identifiers to names for display.\n\n    For most of our supported languages, we only provide one\n    translation, so including the full display name is not necessary\n    to distinguish them. For languages with more than one translation,\n    like Chinese, we do need the additional detail.\n    \"\"\"\n\n    # Deduplicate before sorting.\n    supported_locales = sorted(list(set(config.SUPPORTED_LOCALES)))\n\n    language_locale_counts: DefaultDict[str, int] = collections.defaultdict(int)\n    for code in supported_locales:\n        locale = RequestLocaleInfo(code)\n        language_locale_counts[locale.language] += 1\n\n    locale_map = collections.OrderedDict()\n    for code in supported_locales:\n        if Locale.parse(code) not in usable_locales:\n            continue\n\n        locale = RequestLocaleInfo(code)\n        if language_locale_counts[locale.language] > 1:\n            # Disambiguate translations for this language.\n            locale.use_display_name = True # Untested\n\n        locale_map[str(locale)] = locale\n\n    return locale_map",
          "callGraphToTestedFunction": [
            "map_locale_display_names"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_locale",
        "replayTestFileNames": [],
        "file": "i18n.py",
        "callGraph": [
          {
            "file": "i18n.py",
            "functionName": "get_locale",
            "lines": [
              {
                "startLine": 217,
                "endLine": 217
              },
              {
                "startLine": 227,
                "endLine": 227
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "i18n.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_locale",
            "filePath": "i18n.py",
            "uncoveredLines": [
              {
                "startLine": 217,
                "endLine": 217
              },
              {
                "startLine": 227,
                "endLine": 227
              }
            ]
          },
          "uncoveredFnBody": "def get_locale(config: SecureDropConfig) -> str:\n    \"\"\"\n    Return the best supported locale for a request.\n\n    Get the locale as follows, by order of precedence:\n    - l request argument or session['locale']\n    - browser suggested locale, from the Accept-Languages header\n    - config.DEFAULT_LOCALE\n    - config.FALLBACK_LOCALE\n    \"\"\"\n    preferences: List[str] = []\n    if session and session.get(\"locale\"):\n        preferences.append(session[\"locale\"])\n    if request.args.get(\"l\"):\n        preferences.insert(0, request.args[\"l\"])\n    if not preferences:\n        preferences.extend(get_accepted_languages())\n    preferences.append(config.DEFAULT_LOCALE)\n    preferences.append(FALLBACK_LOCALE)\n\n    locales = current_app.config[\"LOCALES\"]\n    negotiated = negotiate_locale(preferences, locales.keys())\n\n    if not negotiated:\n        raise ValueError(\"No usable locale\") # Untested\n\n    return negotiated",
          "callGraphToTestedFunction": [
            "get_locale"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_or_404",
        "replayTestFileNames": [],
        "file": "journalist_app/api.py",
        "callGraph": [
          {
            "file": "journalist_app/api.py",
            "functionName": "get_or_404",
            "lines": [
              {
                "startLine": 32,
                "endLine": 35
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/api.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_or_404",
            "filePath": "journalist_app/api.py",
            "uncoveredLines": [
              {
                "startLine": 32,
                "endLine": 35
              }
            ]
          },
          "uncoveredFnBody": "def get_or_404(model: db.Model, object_id: str, column: Column) -> db.Model:\n    result = model.query.filter(column == object_id).one_or_none() # Untested\n    if result is None: # Untested\n        abort(404) # Untested\n    return result # Untested",
          "callGraphToTestedFunction": [
            "get_or_404"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_token",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "get_token",
            "lines": [
              {
                "startLine": 44,
                "endLine": 44
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_token",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 44,
                "endLine": 44
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token # Untested\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "get_token"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_lifetime",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "get_lifetime",
            "lines": [
              {
                "startLine": 47,
                "endLine": 47
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_lifetime",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 47,
                "endLine": 47
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime) # Untested\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "get_lifetime"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "set_user",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "set_user",
            "lines": [
              {
                "startLine": 50,
                "endLine": 52
              },
              {
                "startLine": 54,
                "endLine": 55
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_user",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 50,
                "endLine": 52
              },
              {
                "startLine": 54,
                "endLine": 55
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None: # Untested\n            self.user = Journalist.query.get(self.uid) # Untested\n        if self.user is None: # Untested\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "set_user"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_uid",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "get_uid",
            "lines": [
              {
                "startLine": 61,
                "endLine": 61
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_uid",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 61,
                "endLine": 61
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid # Untested\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "get_uid"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "set_uid",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "set_uid",
            "lines": [
              {
                "startLine": 64,
                "endLine": 65
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_uid",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 64,
                "endLine": 65
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None # Untested\n        self.uid = uid # Untested\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "set_uid"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "logged_in",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "logged_in",
            "lines": [
              {
                "startLine": 68,
                "endLine": 68
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "logged_in",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 68,
                "endLine": 68
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None # Untested\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "logged_in"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "destroy",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "destroy",
            "lines": [
              {
                "startLine": 74,
                "endLine": 78
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "destroy",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 74,
                "endLine": 78
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale # Untested\n        self.flash = flash # Untested\n        self.uid = None # Untested\n        self.user = None # Untested\n        self.to_destroy = True # Untested\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "callGraphToTestedFunction": [
            "destroy"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "regenerate",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "regenerate",
            "lines": [
              {
                "startLine": 81,
                "endLine": 81
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "regenerate",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 81,
                "endLine": 81
              }
            ]
          },
          "uncoveredFnBody": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True # Untested",
          "callGraphToTestedFunction": [
            "regenerate"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "init_app",
        "replayTestFileNames": [],
        "file": "journalist_app/sessions.py",
        "callGraph": [
          {
            "file": "journalist_app/sessions.py",
            "functionName": "init_app",
            "lines": [
              {
                "startLine": 241,
                "endLine": 241
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/sessions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "init_app",
            "filePath": "journalist_app/sessions.py",
            "uncoveredLines": [
              {
                "startLine": 241,
                "endLine": 241
              }
            ]
          },
          "uncoveredFnBody": "class Session:\n    def __init__(self, app: Flask, sdconfig: SecureDropConfig) -> None:\n        self.app = app\n        if app is not None:\n            self.init_app(app, sdconfig)\n\n    def init_app(self, app: Flask, sdconfig: SecureDropConfig) -> \"None\":\n        \"\"\"This is used to set up session for your app object.\n        :param app: the Flask app object with proper configuration.\n        \"\"\"\n        app.session_interface = self._get_interface(app, sdconfig)  # type: ignore # Untested\n\n    def _get_interface(self, app: Flask, sdconfig: SecureDropConfig) -> SessionInterface:\n        config = app.config.copy()\n        config.setdefault(\"SESSION_REDIS\", Redis(**sdconfig.REDIS_KWARGS))\n        config.setdefault(\"SESSION_LIFETIME\", 2 * 60 * 60)\n        config.setdefault(\"SESSION_RENEW_COUNT\", 5)\n        config.setdefault(\"SESSION_SIGNER_SALT\", \"session\")\n        config.setdefault(\"SESSION_KEY_PREFIX\", \"session:\")\n        config.setdefault(\"SESSION_HEADER_NAME\", \"authorization\")\n\n        return SessionInterface(\n            config[\"SESSION_LIFETIME\"],\n            config[\"SESSION_RENEW_COUNT\"],\n            config[\"SESSION_REDIS\"],\n            config[\"SESSION_KEY_PREFIX\"],\n            config[\"SESSION_SIGNER_SALT\"],\n            config[\"SESSION_HEADER_NAME\"],\n        )",
          "callGraphToTestedFunction": [
            "init_app"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_source",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "get_source",
            "lines": [
              {
                "startLine": 63,
                "endLine": 63
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_source",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 63,
                "endLine": 63
              }
            ]
          },
          "uncoveredFnBody": "def get_source(filesystem_id: str, include_deleted: bool = False) -> Source:\n    \"\"\"\n    Return the Source object with `filesystem_id`\n\n    If `include_deleted` is False, only sources with a null `deleted_at` will\n    be returned.\n    \"\"\"\n    query = Source.query.filter(Source.filesystem_id == filesystem_id)\n    if not include_deleted:\n        query = query.filter_by(deleted_at=None) # Untested\n    return get_one_or_else(query, current_app.logger, abort)",
          "callGraphToTestedFunction": [
            "get_source"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "make_star_true",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "make_star_true",
            "lines": [
              {
                "startLine": 285,
                "endLine": 287
              },
              {
                "startLine": 289,
                "endLine": 290
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "make_star_true",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 285,
                "endLine": 287
              },
              {
                "startLine": 289,
                "endLine": 290
              }
            ]
          },
          "uncoveredFnBody": "def make_star_true(filesystem_id: str) -> None:\n    source = get_source(filesystem_id) # Untested\n    if source.star: # Untested\n        source.star.starred = True # Untested\n    else:\n        source_star = SourceStar(source)\n        db.session.add(source_star)",
          "callGraphToTestedFunction": [
            "make_star_true"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "col_star",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "col_star",
            "lines": [
              {
                "startLine": 303,
                "endLine": 304
              },
              {
                "startLine": 306,
                "endLine": 307
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "col_star",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 303,
                "endLine": 304
              },
              {
                "startLine": 306,
                "endLine": 307
              }
            ]
          },
          "uncoveredFnBody": "def col_star(cols_selected: List[str]) -> werkzeug.Response:\n    for filesystem_id in cols_selected:\n        make_star_true(filesystem_id)\n\n    db.session.commit() # Untested\n    return redirect(url_for(\"main.index\")) # Untested",
          "callGraphToTestedFunction": [
            "col_star"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "col_un_star",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "col_un_star",
            "lines": [
              {
                "startLine": 311,
                "endLine": 312
              },
              {
                "startLine": 314,
                "endLine": 315
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "col_un_star",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 311,
                "endLine": 312
              },
              {
                "startLine": 314,
                "endLine": 315
              }
            ]
          },
          "uncoveredFnBody": "def col_un_star(cols_selected: List[str]) -> werkzeug.Response:\n    for filesystem_id in cols_selected:\n        make_star_false(filesystem_id)\n\n    db.session.commit() # Untested\n    return redirect(url_for(\"main.index\")) # Untested",
          "callGraphToTestedFunction": [
            "col_un_star"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "set_pending_password",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "set_pending_password",
            "lines": [
              {
                "startLine": 442,
                "endLine": 442
              },
              {
                "startLine": 445,
                "endLine": 446
              },
              {
                "startLine": 448,
                "endLine": 449
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_pending_password",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 442,
                "endLine": 442
              },
              {
                "startLine": 445,
                "endLine": 446
              },
              {
                "startLine": 448,
                "endLine": 449
              }
            ]
          },
          "uncoveredFnBody": "def set_pending_password(for_: Union[Journalist, Literal[\"new\"]], passphrase: str) -> None:\n    \"\"\"\n    The user has requested a password change, but hasn't confirmed it yet.\n\n    NOTE: This mutates the current session and not the database.\n\n    We keep track of the hash so we can verify they are using the password\n    we provided for them. It is expected they hit /new-password  \n    utils.set_diceware_password()\n    \"\"\"\n    hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n    # Include the user's id in the hash to avoid possible collisions in case we're\n    # resetting someone else's password.\n    if isinstance(for_, Journalist):\n        id = str(for_.id)\n    else:  # \"new\"\n        id = for_ # Untested\n    session[f\"pending_password_{id}\"] = hasher.hash(passphrase) # Untested",
          "callGraphToTestedFunction": [
            "set_pending_password"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "col_download_unread",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "col_download_unread",
            "lines": [
              {
                "startLine": 539,
                "endLine": 539
              },
              {
                "startLine": 546,
                "endLine": 548
              },
              {
                "startLine": 550,
                "endLine": 550
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "col_download_unread",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 539,
                "endLine": 539
              },
              {
                "startLine": 546,
                "endLine": 548
              },
              {
                "startLine": 550,
                "endLine": 550
              }
            ]
          },
          "uncoveredFnBody": "def col_download_unread(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"\n    Download all unseen submissions from all selected sources.\n    \"\"\"\n    unseen_submissions = (\n        Submission.query.join(Source)\n        .filter(Source.deleted_at.is_(None), Source.filesystem_id.in_(cols_selected))\n        .filter(~Submission.seen_files.any(), ~Submission.seen_messages.any())\n        .all()\n    )\n\n    if len(unseen_submissions) == 0: # Untested\n        flash(gettext(\"No unread submissions in selected collections.\"), \"error\") # Untested\n        return redirect(url_for(\"main.index\")) # Untested\n\n    return download(\"unread\", unseen_submissions)",
          "callGraphToTestedFunction": [
            "col_download_unread"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "col_download_all",
        "replayTestFileNames": [],
        "file": "journalist_app/utils.py",
        "callGraph": [
          {
            "file": "journalist_app/utils.py",
            "functionName": "col_download_all",
            "lines": [
              {
                "startLine": 555,
                "endLine": 557
              },
              {
                "startLine": 563,
                "endLine": 564
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "journalist_app/utils.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "col_download_all",
            "filePath": "journalist_app/utils.py",
            "uncoveredLines": [
              {
                "startLine": 555,
                "endLine": 557
              },
              {
                "startLine": 563,
                "endLine": 564
              }
            ]
          },
          "uncoveredFnBody": "def col_download_all(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"Download all submissions from all selected sources.\"\"\"\n    submissions: List[Union[Source, Submission]] = [] # Untested\n    for filesystem_id in cols_selected: # Untested\n        id = ( # Untested\n            Source.query.filter(Source.filesystem_id == filesystem_id)\n            .filter_by(deleted_at=None)\n            .one()\n            .id\n        )\n        submissions += Submission.query.filter(Submission.source_id == id).all()\n    return download(\"all\", submissions)",
          "callGraphToTestedFunction": [
            "col_download_all"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "fraction",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "fraction",
            "lines": [
              {
                "startLine": 45,
                "endLine": 45
              },
              {
                "startLine": 49,
                "endLine": 52
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "fraction",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 45,
                "endLine": 45
              },
              {
                "startLine": 49,
                "endLine": 52
              }
            ]
          },
          "uncoveredFnBody": "def fraction(s: str) -> float:\n    \"\"\"\n    Ensures the string is a float between 0 and 1.\n    \"\"\"\n    f = float(s) # Untested\n    if 0 <= f <= 1: # Untested\n        return f # Untested\n    raise ValueError(f\"{s} should be a float between 0 and 1\") # Untested",
          "callGraphToTestedFunction": [
            "fraction"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "non_negative_int",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "non_negative_int",
            "lines": [
              {
                "startLine": 55,
                "endLine": 55
              },
              {
                "startLine": 59,
                "endLine": 62
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "non_negative_int",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 55,
                "endLine": 55
              },
              {
                "startLine": 59,
                "endLine": 62
              }
            ]
          },
          "uncoveredFnBody": "def non_negative_int(s: str) -> int:\n    \"\"\"\n    Ensures the string is a non-negative integer.\n    \"\"\"\n    f = float(s) # Untested\n    if f.is_integer() and f >= 0: # Untested\n        return int(f) # Untested\n    raise ValueError(f\"{s} is not a non-negative integer\") # Untested",
          "callGraphToTestedFunction": [
            "non_negative_int"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "random_bool",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "random_bool",
            "lines": [
              {
                "startLine": 65,
                "endLine": 65
              },
              {
                "startLine": 69,
                "endLine": 69
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "random_bool",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 65,
                "endLine": 65
              },
              {
                "startLine": 69,
                "endLine": 69
              }
            ]
          },
          "uncoveredFnBody": "def random_bool() -> bool:\n    \"\"\"\n    Flips a coin.\n    \"\"\"\n    return secrets.choice((True, False)) # Untested",
          "callGraphToTestedFunction": [
            "random_bool"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "random_chars",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "random_chars",
            "lines": [
              {
                "startLine": 72,
                "endLine": 72
              },
              {
                "startLine": 76,
                "endLine": 76
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "random_chars",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 72,
                "endLine": 72
              },
              {
                "startLine": 76,
                "endLine": 76
              }
            ]
          },
          "uncoveredFnBody": "def random_chars(count: int, chars: str = string.ascii_letters) -> str:\n    \"\"\"\n    Returns a random string of len characters from the supplied list.\n    \"\"\"\n    return \"\".join([secrets.choice(chars) for _ in range(count)]) # Untested",
          "callGraphToTestedFunction": [
            "random_chars"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "default_journalist_count",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "default_journalist_count",
            "lines": [
              {
                "startLine": 105,
                "endLine": 106
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "default_journalist_count",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 105,
                "endLine": 106
              }
            ]
          },
          "uncoveredFnBody": "def default_journalist_count() -> str: # Untested\n    return os.environ.get(\"NUM_JOURNALISTS\", \"0\") # Untested",
          "callGraphToTestedFunction": [
            "default_journalist_count"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "default_source_count",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "default_source_count",
            "lines": [
              {
                "startLine": 109,
                "endLine": 110
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "default_source_count",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 109,
                "endLine": 110
              }
            ]
          },
          "uncoveredFnBody": "def default_source_count() -> str: # Untested\n    return os.environ.get(\"NUM_SOURCES\", \"3\") # Untested",
          "callGraphToTestedFunction": [
            "default_source_count"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "set_source_count",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "set_source_count",
            "lines": [
              {
                "startLine": 113,
                "endLine": 113
              },
              {
                "startLine": 122,
                "endLine": 124
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_source_count",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 113,
                "endLine": 113
              },
              {
                "startLine": 122,
                "endLine": 124
              }
            ]
          },
          "uncoveredFnBody": "def set_source_count(s: str) -> int:\n    \"\"\"\n    Sets the source count from command line arguments.\n\n    The --source-count argument can be either a positive integer or\n    the special string \"ALL\", which will result in a number of sources\n    that can demonstrate all of the special strings we want to test,\n    if each source uses two of the strings.\n    \"\"\"\n    if s == \"ALL\": # Untested\n        return math.ceil(len(strings) / 2) # Untested\n    return non_negative_int(s) # Untested",
          "callGraphToTestedFunction": [
            "set_source_count"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "record_source_interaction",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "record_source_interaction",
            "lines": [
              {
                "startLine": 174,
                "endLine": 174
              },
              {
                "startLine": 178,
                "endLine": 181
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "record_source_interaction",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 174,
                "endLine": 174
              },
              {
                "startLine": 178,
                "endLine": 181
              }
            ]
          },
          "uncoveredFnBody": "def record_source_interaction(source: Source) -> None:\n    \"\"\"\n    Updates the source's interaction count, pending status, and timestamp.\n    \"\"\"\n    source.interaction_count += 1 # Untested\n    source.pending = False # Untested\n    source.last_updated = datetime.datetime.utcnow() # Untested\n    db.session.flush() # Untested",
          "callGraphToTestedFunction": [
            "record_source_interaction"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "star_source",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "star_source",
            "lines": [
              {
                "startLine": 292,
                "endLine": 292
              },
              {
                "startLine": 296,
                "endLine": 298
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "star_source",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 292,
                "endLine": 292
              },
              {
                "startLine": 296,
                "endLine": 298
              }
            ]
          },
          "uncoveredFnBody": "def star_source(source: Source) -> None:\n    \"\"\"\n    Adds a SourceStar record for the source.\n    \"\"\"\n    star = SourceStar(source, True) # Untested\n    db.session.add(star) # Untested\n    db.session.commit() # Untested",
          "callGraphToTestedFunction": [
            "star_source"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_journalists",
        "replayTestFileNames": [],
        "file": "loaddata.py",
        "callGraph": [
          {
            "file": "loaddata.py",
            "functionName": "add_journalists",
            "lines": [
              {
                "startLine": 337,
                "endLine": 340
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "loaddata.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_journalists",
            "filePath": "loaddata.py",
            "uncoveredLines": [
              {
                "startLine": 337,
                "endLine": 340
              }
            ]
          },
          "uncoveredFnBody": "def add_journalists(args: argparse.Namespace) -> None: # Untested\n    total = args.journalist_count # Untested\n    for i in range(1, total + 1): # Untested\n        add_journalist(username=f\"journalist{str(i)}\", progress=(i, total)) # Untested",
          "callGraphToTestedFunction": [
            "add_journalists"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "obtain_input",
        "replayTestFileNames": [],
        "file": "manage.py",
        "callGraph": [
          {
            "file": "manage.py",
            "functionName": "obtain_input",
            "lines": [
              {
                "startLine": 48,
                "endLine": 48
              },
              {
                "startLine": 51,
                "endLine": 51
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "manage.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "obtain_input",
            "filePath": "manage.py",
            "uncoveredLines": [
              {
                "startLine": 48,
                "endLine": 48
              },
              {
                "startLine": 51,
                "endLine": 51
              }
            ]
          },
          "uncoveredFnBody": "def obtain_input(text: str) -> str:\n    \"\"\"Wrapper for testability as suggested in\n    https://github.com/pytest-dev/pytest/issues/1598#issuecomment-224761877\"\"\"\n    return input(text) # Untested",
          "callGraphToTestedFunction": [
            "obtain_input"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_admin",
        "replayTestFileNames": [],
        "file": "manage.py",
        "callGraph": [
          {
            "file": "manage.py",
            "functionName": "add_admin",
            "lines": [
              {
                "startLine": 121,
                "endLine": 122
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "manage.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_admin",
            "filePath": "manage.py",
            "uncoveredLines": [
              {
                "startLine": 121,
                "endLine": 122
              }
            ]
          },
          "uncoveredFnBody": "def add_admin(args: argparse.Namespace) -> int: # Untested\n    return _add_user(is_admin=True) # Untested",
          "callGraphToTestedFunction": [
            "add_admin"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_journalist",
        "replayTestFileNames": [],
        "file": "manage.py",
        "callGraph": [
          {
            "file": "manage.py",
            "functionName": "add_journalist",
            "lines": [
              {
                "startLine": 125,
                "endLine": 126
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "manage.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_journalist",
            "filePath": "manage.py",
            "uncoveredLines": [
              {
                "startLine": 125,
                "endLine": 126
              }
            ]
          },
          "uncoveredFnBody": "def add_journalist(args: argparse.Namespace) -> int: # Untested\n    return _add_user() # Untested",
          "callGraphToTestedFunction": [
            "add_journalist"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "find_pending_sources",
        "replayTestFileNames": [],
        "file": "management/sources.py",
        "callGraph": [
          {
            "file": "management/sources.py",
            "functionName": "find_pending_sources",
            "lines": [
              {
                "startLine": 29,
                "endLine": 29
              },
              {
                "startLine": 33,
                "endLine": 34
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/sources.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "find_pending_sources",
            "filePath": "management/sources.py",
            "uncoveredLines": [
              {
                "startLine": 29,
                "endLine": 29
              },
              {
                "startLine": 33,
                "endLine": 34
              }
            ]
          },
          "uncoveredFnBody": "def find_pending_sources(keep_most_recent: int) -> List[Source]:\n    \"\"\"\n    Finds all sources that are marked as pending\n    \"\"\"\n    with app_context(): # Untested\n        return ( # Untested\n            Source.query.filter_by(pending=True)\n            .order_by(Source.id.desc())\n            .offset(keep_most_recent)\n            .all()\n        )",
          "callGraphToTestedFunction": [
            "find_pending_sources"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_check_db_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_check_db_disconnect_parser",
            "lines": [
              {
                "startLine": 184,
                "endLine": 185
              },
              {
                "startLine": 189,
                "endLine": 189
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_check_db_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 184,
                "endLine": 185
              },
              {
                "startLine": 189,
                "endLine": 189
              }
            ]
          },
          "uncoveredFnBody": "def add_check_db_disconnect_parser(subps: _SubParsersAction) -> None: # Untested\n    check_db_disconnect_subp = subps.add_parser( # Untested\n        \"check-disconnected-db-submissions\",\n        help=\"Check for submissions that exist in the database but not the filesystem.\",\n    )\n    check_db_disconnect_subp.set_defaults(func=check_for_disconnected_db_submissions)",
          "callGraphToTestedFunction": [
            "add_check_db_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_check_fs_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_check_fs_disconnect_parser",
            "lines": [
              {
                "startLine": 192,
                "endLine": 193
              },
              {
                "startLine": 197,
                "endLine": 197
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_check_fs_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 192,
                "endLine": 193
              },
              {
                "startLine": 197,
                "endLine": 197
              }
            ]
          },
          "uncoveredFnBody": "def add_check_fs_disconnect_parser(subps: _SubParsersAction) -> None: # Untested\n    check_fs_disconnect_subp = subps.add_parser( # Untested\n        \"check-disconnected-fs-submissions\",\n        help=\"Check for submissions that exist in the filesystem but not in the database.\",\n    )\n    check_fs_disconnect_subp.set_defaults(func=check_for_disconnected_fs_submissions)",
          "callGraphToTestedFunction": [
            "add_check_fs_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_delete_db_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_delete_db_disconnect_parser",
            "lines": [
              {
                "startLine": 200,
                "endLine": 201
              },
              {
                "startLine": 205,
                "endLine": 206
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_delete_db_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 200,
                "endLine": 201
              },
              {
                "startLine": 205,
                "endLine": 206
              }
            ]
          },
          "uncoveredFnBody": "def add_delete_db_disconnect_parser(subps: _SubParsersAction) -> None:\n    delete_db_disconnect_subp = subps.add_parser(\n        \"delete-disconnected-db-submissions\",\n        help=\"Delete submissions that exist in the database but not the filesystem.\",\n    )\n    delete_db_disconnect_subp.set_defaults(func=delete_disconnected_db_submissions) # Untested\n    delete_db_disconnect_subp.add_argument( # Untested\n        \"--force\", action=\"store_true\", help=\"Do not ask for confirmation.\"\n    )",
          "callGraphToTestedFunction": [
            "add_delete_db_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_delete_fs_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_delete_fs_disconnect_parser",
            "lines": [
              {
                "startLine": 211,
                "endLine": 212
              },
              {
                "startLine": 216,
                "endLine": 217
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_delete_fs_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 211,
                "endLine": 212
              },
              {
                "startLine": 216,
                "endLine": 217
              }
            ]
          },
          "uncoveredFnBody": "def add_delete_fs_disconnect_parser(subps: _SubParsersAction) -> None:\n    delete_fs_disconnect_subp = subps.add_parser(\n        \"delete-disconnected-fs-submissions\",\n        help=\"Delete submissions that exist in the filesystem but not the database.\",\n    )\n    delete_fs_disconnect_subp.set_defaults(func=delete_disconnected_fs_submissions) # Untested\n    delete_fs_disconnect_subp.add_argument( # Untested\n        \"--force\", action=\"store_true\", help=\"Do not ask for confirmation.\"\n    )",
          "callGraphToTestedFunction": [
            "add_delete_fs_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_list_db_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_list_db_disconnect_parser",
            "lines": [
              {
                "startLine": 222,
                "endLine": 223
              },
              {
                "startLine": 227,
                "endLine": 227
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_list_db_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 222,
                "endLine": 223
              },
              {
                "startLine": 227,
                "endLine": 227
              }
            ]
          },
          "uncoveredFnBody": "def add_list_db_disconnect_parser(subps: _SubParsersAction) -> None: # Untested\n    list_db_disconnect_subp = subps.add_parser( # Untested\n        \"list-disconnected-db-submissions\",\n        help=\"List submissions that exist in the database but not the filesystem.\",\n    )\n    list_db_disconnect_subp.set_defaults(func=list_disconnected_db_submissions)",
          "callGraphToTestedFunction": [
            "add_list_db_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_list_fs_disconnect_parser",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_list_fs_disconnect_parser",
            "lines": [
              {
                "startLine": 230,
                "endLine": 231
              },
              {
                "startLine": 235,
                "endLine": 235
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_list_fs_disconnect_parser",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 230,
                "endLine": 231
              },
              {
                "startLine": 235,
                "endLine": 235
              }
            ]
          },
          "uncoveredFnBody": "def add_list_fs_disconnect_parser(subps: _SubParsersAction) -> None: # Untested\n    list_fs_disconnect_subp = subps.add_parser( # Untested\n        \"list-disconnected-fs-submissions\",\n        help=\"List submissions that exist in the filesystem but not the database.\",\n    )\n    list_fs_disconnect_subp.set_defaults(func=list_disconnected_fs_submissions)",
          "callGraphToTestedFunction": [
            "add_list_fs_disconnect_parser"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "add_were_there_submissions_today",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "add_were_there_submissions_today",
            "lines": [
              {
                "startLine": 238,
                "endLine": 239
              },
              {
                "startLine": 243,
                "endLine": 243
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "add_were_there_submissions_today",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 238,
                "endLine": 239
              },
              {
                "startLine": 243,
                "endLine": 243
              }
            ]
          },
          "uncoveredFnBody": "def add_were_there_submissions_today(subps: _SubParsersAction) -> None: # Untested\n    parser = subps.add_parser( # Untested\n        \"were-there-submissions-today\",\n        help=(\"Update the file indicating \" \"whether submissions were received in the past 24h.\"),\n    )\n    parser.set_defaults(func=were_there_submissions_today)",
          "callGraphToTestedFunction": [
            "add_were_there_submissions_today"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "collection",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "collection",
            "lines": [
              {
                "startLine": 107,
                "endLine": 111
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "collection",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 107,
                "endLine": 111
              }
            ]
          },
          "uncoveredFnBody": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = [] # Untested\n        collection.extend(self.submissions) # Untested\n        collection.extend(self.replies) # Untested\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0])) # Untested\n        return collection # Untested\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred:\n            starred = True\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
          "callGraphToTestedFunction": [
            "collection"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_file",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "is_file",
            "lines": [
              {
                "startLine": 196,
                "endLine": 196
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_file",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 196,
                "endLine": 196
              }
            ]
          },
          "uncoveredFnBody": "class Submission(db.Model):\n    MAX_MESSAGE_LEN = 100000\n\n    __tablename__ = \"submissions\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    source_id = Column(Integer, ForeignKey(\"sources.id\"))\n    source = relationship(\"Source\", backref=backref(\"submissions\", order_by=id, cascade=\"delete\"))\n\n    filename = Column(String(255), nullable=False)\n    size = Column(Integer, nullable=False)\n    downloaded = Column(Boolean, default=False)\n    \"\"\"\n    The checksum of the encrypted file on disk.\n    Format: $hash_name:$hex_encoded_hash_value\n    Example: sha256:05fa5efd7d1b608ac1fbdf19a61a5a439d05b05225e81faa63fdd188296b614a\n    \"\"\"\n    checksum = Column(String(255))\n\n    def __init__(self, source: Source, filename: str, storage: Storage) -> None:\n        self.source_id = source.id\n        self.filename = filename\n        self.uuid = str(uuid.uuid4())\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size\n\n    def __repr__(self) -> str:\n        return f\"<Submission {self.filename!r}>\"\n\n    @property\n    def is_file(self) -> bool:\n        return self.filename.endswith(\"doc.gz.gpg\") or self.filename.endswith(\"doc.zip.gpg\") # Untested\n\n    @property\n    def is_message(self) -> bool:\n        return self.filename.endswith(\"msg.gpg\")\n\n    def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = {\n            f.journalist.uuid\n            for f in SeenFile.query.filter(SeenFile.file_id == self.id)\n            if f.journalist\n        }\n        seen_by.update(\n            {\n                m.journalist.uuid\n                for m in SeenMessage.query.filter(SeenMessage.message_id == self.id)\n                if m.journalist\n            }\n        )\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"submission_url\": (\n                url_for(\n                    \"api.single_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"is_file\": self.is_file,\n            \"is_message\": self.is_message,\n            \"is_read\": self.seen,\n            \"uuid\": self.uuid,\n            \"download_url\": (\n                url_for(\n                    \"api.download_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"seen_by\": list(seen_by),\n        }\n\n    @property\n    def seen(self) -> bool:\n        \"\"\"\n        If the submission has been downloaded or seen by any journalist, then the submission is\n        considered seen.\n        \"\"\"\n        return bool(self.downloaded or self.seen_files.count() or self.seen_messages.count())",
          "callGraphToTestedFunction": [
            "is_file"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_message",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "is_message",
            "lines": [
              {
                "startLine": 200,
                "endLine": 200
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_message",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 200,
                "endLine": 200
              }
            ]
          },
          "uncoveredFnBody": "class Submission(db.Model):\n    MAX_MESSAGE_LEN = 100000\n\n    __tablename__ = \"submissions\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    source_id = Column(Integer, ForeignKey(\"sources.id\"))\n    source = relationship(\"Source\", backref=backref(\"submissions\", order_by=id, cascade=\"delete\"))\n\n    filename = Column(String(255), nullable=False)\n    size = Column(Integer, nullable=False)\n    downloaded = Column(Boolean, default=False)\n    \"\"\"\n    The checksum of the encrypted file on disk.\n    Format: $hash_name:$hex_encoded_hash_value\n    Example: sha256:05fa5efd7d1b608ac1fbdf19a61a5a439d05b05225e81faa63fdd188296b614a\n    \"\"\"\n    checksum = Column(String(255))\n\n    def __init__(self, source: Source, filename: str, storage: Storage) -> None:\n        self.source_id = source.id\n        self.filename = filename\n        self.uuid = str(uuid.uuid4())\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size\n\n    def __repr__(self) -> str:\n        return f\"<Submission {self.filename!r}>\"\n\n    @property\n    def is_file(self) -> bool:\n        return self.filename.endswith(\"doc.gz.gpg\") or self.filename.endswith(\"doc.zip.gpg\")\n\n    @property\n    def is_message(self) -> bool:\n        return self.filename.endswith(\"msg.gpg\") # Untested\n\n    def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = {\n            f.journalist.uuid\n            for f in SeenFile.query.filter(SeenFile.file_id == self.id)\n            if f.journalist\n        }\n        seen_by.update(\n            {\n                m.journalist.uuid\n                for m in SeenMessage.query.filter(SeenMessage.message_id == self.id)\n                if m.journalist\n            }\n        )\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"submission_url\": (\n                url_for(\n                    \"api.single_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"is_file\": self.is_file,\n            \"is_message\": self.is_message,\n            \"is_read\": self.seen,\n            \"uuid\": self.uuid,\n            \"download_url\": (\n                url_for(\n                    \"api.download_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"seen_by\": list(seen_by),\n        }\n\n    @property\n    def seen(self) -> bool:\n        \"\"\"\n        If the submission has been downloaded or seen by any journalist, then the submission is\n        considered seen.\n        \"\"\"\n        return bool(self.downloaded or self.seen_files.count() or self.seen_messages.count())",
          "callGraphToTestedFunction": [
            "is_message"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "seen",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "seen",
            "lines": [
              {
                "startLine": 252,
                "endLine": 252
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "seen",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 252,
                "endLine": 252
              }
            ]
          },
          "uncoveredFnBody": "class Submission(db.Model):\n    MAX_MESSAGE_LEN = 100000\n\n    __tablename__ = \"submissions\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    source_id = Column(Integer, ForeignKey(\"sources.id\"))\n    source = relationship(\"Source\", backref=backref(\"submissions\", order_by=id, cascade=\"delete\"))\n\n    filename = Column(String(255), nullable=False)\n    size = Column(Integer, nullable=False)\n    downloaded = Column(Boolean, default=False)\n    \"\"\"\n    The checksum of the encrypted file on disk.\n    Format: $hash_name:$hex_encoded_hash_value\n    Example: sha256:05fa5efd7d1b608ac1fbdf19a61a5a439d05b05225e81faa63fdd188296b614a\n    \"\"\"\n    checksum = Column(String(255))\n\n    def __init__(self, source: Source, filename: str, storage: Storage) -> None:\n        self.source_id = source.id\n        self.filename = filename\n        self.uuid = str(uuid.uuid4())\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size\n\n    def __repr__(self) -> str:\n        return f\"<Submission {self.filename!r}>\"\n\n    @property\n    def is_file(self) -> bool:\n        return self.filename.endswith(\"doc.gz.gpg\") or self.filename.endswith(\"doc.zip.gpg\")\n\n    @property\n    def is_message(self) -> bool:\n        return self.filename.endswith(\"msg.gpg\")\n\n    def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = {\n            f.journalist.uuid\n            for f in SeenFile.query.filter(SeenFile.file_id == self.id)\n            if f.journalist\n        }\n        seen_by.update(\n            {\n                m.journalist.uuid\n                for m in SeenMessage.query.filter(SeenMessage.message_id == self.id)\n                if m.journalist\n            }\n        )\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"submission_url\": (\n                url_for(\n                    \"api.single_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"is_file\": self.is_file,\n            \"is_message\": self.is_message,\n            \"is_read\": self.seen,\n            \"uuid\": self.uuid,\n            \"download_url\": (\n                url_for(\n                    \"api.download_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"seen_by\": list(seen_by),\n        }\n\n    @property\n    def seen(self) -> bool:\n        \"\"\"\n        If the submission has been downloaded or seen by any journalist, then the submission is\n        considered seen.\n        \"\"\"\n        return bool(self.downloaded or self.seen_files.count() or self.seen_messages.count()) # Untested",
          "callGraphToTestedFunction": [
            "seen"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "check_username_acceptable",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "check_username_acceptable",
            "lines": [
              {
                "startLine": 487,
                "endLine": 487
              },
              {
                "startLine": 495,
                "endLine": 495
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_username_acceptable",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 487,
                "endLine": 487
              },
              {
                "startLine": 495,
                "endLine": 495
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException( # Untested\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "check_username_acceptable"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "check_name_acceptable",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "check_name_acceptable",
            "lines": [
              {
                "startLine": 505,
                "endLine": 506
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_name_acceptable",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 505,
                "endLine": 506
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN: # Untested\n            raise InvalidNameLength() # Untested\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "check_name_acceptable"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "check_password_acceptable",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "check_password_acceptable",
            "lines": [
              {
                "startLine": 512,
                "endLine": 512
              },
              {
                "startLine": 516,
                "endLine": 516
              },
              {
                "startLine": 520,
                "endLine": 520
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_password_acceptable",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 512,
                "endLine": 512
              },
              {
                "startLine": 516,
                "endLine": 516
              },
              {
                "startLine": 520,
                "endLine": 520
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword() # Untested\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "check_password_acceptable"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "regenerate_totp_shared_secret",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "regenerate_totp_shared_secret",
            "lines": [
              {
                "startLine": 581,
                "endLine": 581
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "regenerate_totp_shared_secret",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 581,
                "endLine": 581
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32() # Untested\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "regenerate_totp_shared_secret"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "set_hotp_secret",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "set_hotp_secret",
            "lines": [
              {
                "startLine": 584,
                "endLine": 584
              },
              {
                "startLine": 587,
                "endLine": 588
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_hotp_secret",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 584,
                "endLine": 584
              },
              {
                "startLine": 587,
                "endLine": 588
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False # Untested\n        self.hotp_counter = 0 # Untested\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "set_hotp_secret"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "totp",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "totp",
            "lines": [
              {
                "startLine": 592,
                "endLine": 593
              },
              {
                "startLine": 595,
                "endLine": 595
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "totp",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 592,
                "endLine": 593
              },
              {
                "startLine": 595,
                "endLine": 595
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp: # Untested\n            return two_factor.TOTP(self.otp_secret) # Untested\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "totp"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "hotp",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "hotp",
            "lines": [
              {
                "startLine": 599,
                "endLine": 600
              },
              {
                "startLine": 602,
                "endLine": 602
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "hotp",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 599,
                "endLine": 600
              },
              {
                "startLine": 602,
                "endLine": 602
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp: # Untested\n            return two_factor.HOTP(self.otp_secret) # Untested\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "hotp"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "formatted_otp_secret",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "formatted_otp_secret",
            "lines": [
              {
                "startLine": 606,
                "endLine": 606
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "formatted_otp_secret",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 606,
                "endLine": 606
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret) # Untested\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "formatted_otp_secret"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_deleted_user",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "is_deleted_user",
            "lines": [
              {
                "startLine": 706,
                "endLine": 706
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_deleted_user",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 706,
                "endLine": 706
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\" # Untested\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "is_deleted_user"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "status",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_logger.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_logger.py",
            "functionName": "status",
            "lines": [
              {
                "startLine": 32,
                "endLine": 32
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_logger.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "status",
            "filePath": "pretty_bad_protocol/_logger.py",
            "uncoveredLines": [
              {
                "startLine": 32,
                "endLine": 32
              }
            ]
          },
          "uncoveredFnBody": "def status(self, message, *args, **kwargs):  # type: ignore[no-untyped-def]\n    \"\"\"LogRecord for GnuPG internal status messages.\"\"\"\n    if self.isEnabledFor(GNUPG_STATUS_LEVEL):\n        self._log(GNUPG_STATUS_LEVEL, message, args, **kwargs) # Untested",
          "callGraphToTestedFunction": [
            "status"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "default_preference_list",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_meta.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_meta.py",
            "functionName": "default_preference_list",
            "lines": [
              {
                "startLine": 331,
                "endLine": 333
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_meta.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "default_preference_list",
            "filePath": "pretty_bad_protocol/_meta.py",
            "uncoveredLines": [
              {
                "startLine": 331,
                "endLine": 333
              }
            ]
          },
          "uncoveredFnBody": "class GPGBase:\n    \"\"\"Base class for storing properties and controlling process initialisation.\n\n    :const _result_map: A *dict* containing classes from\n                        :mod:`~gnupg._parsers`, used for parsing results\n                        obtained from GnuPG commands.\n    :const _decode_errors: How to handle encoding errors.\n    \"\"\"\n\n    __metaclass__ = GPGMeta\n    _decode_errors = \"strict\"\n    _result_map = {\n        \"crypt\": _parsers.Crypt,\n        \"delete\": _parsers.DeleteResult,\n        \"generate\": _parsers.GenKey,\n        \"import\": _parsers.ImportResult,\n        \"export\": _parsers.ExportResult,\n        \"list\": _parsers.ListKeys,\n        \"sign\": _parsers.Sign,\n        \"verify\": _parsers.Verify,\n        \"expire\": _parsers.KeyExpirationResult,\n        \"signing\": _parsers.KeySigningResult,\n        \"packets\": _parsers.ListPackets,\n    }\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()\n\n    def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                os.environ = environment\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)\n\n    @property\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs\n\n    @default_preference_list.setter\n    def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs) # Untested\n        if prefs is not None: # Untested\n            self._prefs = prefs # Untested\n\n    @default_preference_list.deleter\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"\n\n    @property\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver\n\n    @keyserver.setter\n    def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location\n\n    @keyserver.deleter\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n\n    def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir\n\n    def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd\n\n    homedir = _util.InheritableProperty(_homedir_getter, _homedir_setter)\n\n    def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys\n\n    def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd\n\n    _generated_keys = _util.InheritableProperty(_generated_keys_getter, _generated_keys_setter)\n\n    def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)\n\n    def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring:\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd\n\n    def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )\n\n    def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` \n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)\n\n    def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))\n\n    def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose\n\n    def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()\n\n    def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result\n\n    def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result\n\n    def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result\n\n    def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result\n\n    def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "callGraphToTestedFunction": [
            "default_preference_list"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "keyserver",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_meta.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_meta.py",
            "functionName": "keyserver",
            "lines": [
              {
                "startLine": 366,
                "endLine": 366
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_meta.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "keyserver",
            "filePath": "pretty_bad_protocol/_meta.py",
            "uncoveredLines": [
              {
                "startLine": 366,
                "endLine": 366
              }
            ]
          },
          "uncoveredFnBody": "class GPGBase:\n    \"\"\"Base class for storing properties and controlling process initialisation.\n\n    :const _result_map: A *dict* containing classes from\n                        :mod:`~gnupg._parsers`, used for parsing results\n                        obtained from GnuPG commands.\n    :const _decode_errors: How to handle encoding errors.\n    \"\"\"\n\n    __metaclass__ = GPGMeta\n    _decode_errors = \"strict\"\n    _result_map = {\n        \"crypt\": _parsers.Crypt,\n        \"delete\": _parsers.DeleteResult,\n        \"generate\": _parsers.GenKey,\n        \"import\": _parsers.ImportResult,\n        \"export\": _parsers.ExportResult,\n        \"list\": _parsers.ListKeys,\n        \"sign\": _parsers.Sign,\n        \"verify\": _parsers.Verify,\n        \"expire\": _parsers.KeyExpirationResult,\n        \"signing\": _parsers.KeySigningResult,\n        \"packets\": _parsers.ListPackets,\n    }\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()\n\n    def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                os.environ = environment\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)\n\n    @property\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs\n\n    @default_preference_list.setter\n    def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs)\n        if prefs is not None:\n            self._prefs = prefs\n\n    @default_preference_list.deleter\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"\n\n    @property\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver\n\n    @keyserver.setter\n    def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location # Untested\n\n    @keyserver.deleter\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n\n    def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir\n\n    def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd\n\n    homedir = _util.InheritableProperty(_homedir_getter, _homedir_setter)\n\n    def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys\n\n    def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd\n\n    _generated_keys = _util.InheritableProperty(_generated_keys_getter, _generated_keys_setter)\n\n    def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)\n\n    def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring:\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd\n\n    def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )\n\n    def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` \n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)\n\n    def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))\n\n    def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose\n\n    def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()\n\n    def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result\n\n    def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result\n\n    def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result\n\n    def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result\n\n    def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "callGraphToTestedFunction": [
            "keyserver"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "nodata",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "nodata",
            "lines": [
              {
                "startLine": 848,
                "endLine": 848
              },
              {
                "startLine": 854,
                "endLine": 856
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "nodata",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 848,
                "endLine": 848
              },
              {
                "startLine": 854,
                "endLine": 856
              }
            ]
          },
          "uncoveredFnBody": "def nodata(status_code):  # type: ignore[no-untyped-def]\n    \"\"\"Translate NODATA status codes from GnuPG to messages.\"\"\"\n    lookup = {\n        \"1\": \"No armored data.\",\n        \"2\": \"Expected a packet but did not find one.\",\n        \"3\": \"Invalid packet found, this may indicate a non OpenPGP message.\",\n        \"4\": \"Signature expected but not found.\",\n    }\n    for key, value in lookup.items(): # Untested\n        if str(status_code) == key: # Untested\n            return value # Untested",
          "callGraphToTestedFunction": [
            "nodata"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "progress",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "progress",
            "lines": [
              {
                "startLine": 861,
                "endLine": 861
              },
              {
                "startLine": 871,
                "endLine": 873
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "progress",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 861,
                "endLine": 861
              },
              {
                "startLine": 871,
                "endLine": 873
              }
            ]
          },
          "uncoveredFnBody": "def progress(status_code):  # type: ignore[no-untyped-def]\n    \"\"\"Translate PROGRESS status codes from GnuPG to messages.\"\"\"\n    lookup = {\n        \"pk_dsa\": \"DSA key generation\",\n        \"pk_elg\": \"Elgamal key generation\",\n        \"primegen\": \"Prime generation\",\n        \"need_entropy\": \"Waiting for new entropy in the RNG\",\n        \"tick\": \"Generic tick without any special meaning - still working.\",\n        \"starting_agent\": \"A gpg-agent was started.\",\n        \"learncard\": \"gpg-agent or gpgsm is learning the smartcard data.\",\n        \"card_busy\": \"A smartcard is still working.\",\n    }\n    for key, value in lookup.items(): # Untested\n        if str(status_code) == key: # Untested\n            return value # Untested",
          "callGraphToTestedFunction": [
            "progress"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "gpg_interactive_input",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "gpg_interactive_input",
            "lines": [
              {
                "startLine": 907,
                "endLine": 907
              },
              {
                "startLine": 909,
                "endLine": 912
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "gpg_interactive_input",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 907,
                "endLine": 907
              },
              {
                "startLine": 909,
                "endLine": 912
              }
            ]
          },
          "uncoveredFnBody": "class KeyExpirationInterface:\n    \"\"\"Interface that guards against misuse of --edit-key combined with --command-fd\"\"\"\n\n    def __init__(self, expiration_time, passphrase=None):  # type: ignore[no-untyped-def]\n        self._passphrase = passphrase\n        self._expiration_time = expiration_time\n        self._clean_key_expiration_option()\n\n    def _clean_key_expiration_option(self):  # type: ignore[no-untyped-def]\n        \"\"\"validates the expiration option supplied\"\"\"\n        allowed_entry = re.findall(r\"^(\\d+)(|w|m|y)$\", self._expiration_time)\n        if not allowed_entry:\n            raise UsageError(\"Key expiration option: %s is not valid\" % self._expiration_time)\n\n    def _input_passphrase(self, _input):  # type: ignore[no-untyped-def]\n        if self._passphrase:\n            return f\"{_input}{self._passphrase}\\n\"\n        return _input\n\n    def _main_key_command(self):  # type: ignore[no-untyped-def]\n        main_key_input = \"expire\\n%s\\n\" % self._expiration_time\n        return self._input_passphrase(main_key_input)\n\n    def _sub_key_command(self, sub_key_number):  # type: ignore[no-untyped-def]\n        sub_key_input = \"key %d\\nexpire\\n%s\\n\" % (sub_key_number, self._expiration_time)\n        return self._input_passphrase(sub_key_input)\n\n    def gpg_interactive_input(self, sub_keys_number):  # type: ignore[no-untyped-def]\n        \"\"\"processes series of inputs normally supplied on --edit-key but passed through stdin\n        this ensures that no other --edit-key command is actually passing through.\n        \"\"\"\n        deselect_sub_key = \"key 0\\n\"\n\n        _input = self._main_key_command() # Untested\n        for sub_key_number in range(1, sub_keys_number + 1): # Untested\n            _input += self._sub_key_command(sub_key_number) + deselect_sub_key # Untested\n        return \"%ssave\\n\" % _input # Untested",
          "callGraphToTestedFunction": [
            "gpg_interactive_input"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "rev",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "rev",
            "lines": [
              {
                "startLine": 1251,
                "endLine": 1251
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "rev",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 1251,
                "endLine": 1251
              }
            ]
          },
          "uncoveredFnBody": "class ListKeys(list):\n    \"\"\"Handle status messages for --list-keys.\n\n    Handles pub and uid (relating the latter to the former).  Don't care about\n    the following attributes/status messages (from doc/DETAILS):\n\n    |  crt = X.509 certificate\n    |  crs = X.509 certificate and private key available\n    |  ssb = secret subkey (secondary key)\n    |  uat = user attribute (same as user id except for field 10).\n    |  pkd = public key data (special field format, see below)\n    |  grp = reserved for gpgsm\n    |  rvk = revocation key\n    \"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        super().__init__()\n        self._gpg = gpg\n        self.curkey = None\n        self.curuid = None\n        self.fingerprints = []\n        self.uids = []\n        self.sigs = {}\n        self.certs = {}\n        self.revs = {}\n\n    def key(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        self.curkey = {}\n        for i in range(len(vars)):\n            self.curkey[vars[i]] = args[i]\n        self.curkey[\"uids\"] = []\n        self.curkey[\"sigs\"] = {}\n        self.curkey[\"rev\"] = {}\n        if self.curkey[\"uid\"]:\n            self.curuid = self.curkey[\"uid\"]\n            self.curkey[\"uids\"].append(self.curuid)\n            self.sigs[self.curuid] = set()\n            self.certs[self.curuid] = set()\n            self.revs[self.curuid] = set()\n            self.curkey[\"sigs\"][self.curuid] = []\n        del self.curkey[\"uid\"]\n        self.curkey[\"subkeys\"] = []\n        self.append(self.curkey)\n\n    pub = sec = key\n\n    def fpr(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"fingerprint\"] = args[9]\n        self.fingerprints.append(args[9])\n\n    def uid(self, args):  # type: ignore[no-untyped-def]\n        uid = args[9]\n        uid = ESCAPE_PATTERN.sub(lambda m: chr(int(m.group(1), 16)), uid)\n        self.curkey[\"uids\"].append(uid)\n        self.curuid = uid\n        self.curkey[\"sigs\"][uid] = []\n        self.sigs[uid] = set()\n        self.certs[uid] = set()\n        self.uids.append(uid)\n\n    def sig(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        sig = {}\n        for i in range(len(vars)):\n            sig[vars[i]] = args[i]\n        self.curkey[\"sigs\"][self.curuid].append(sig)\n        self.sigs[self.curuid].add(sig[\"keyid\"])\n        if sig[\"trust\"] == \"!\":\n            self.certs[self.curuid].add(sig[\"keyid\"])\n\n    def sub(self, args):  # type: ignore[no-untyped-def]\n        subkey = [args[4], args[11]]\n        self.curkey[\"subkeys\"].append(subkey)\n\n    def rev(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"rev\"] = {\"keyid\": args[4], \"revtime\": args[5], \"uid\": self.curuid} # Untested\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        pass",
          "callGraphToTestedFunction": [
            "rev"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "summary",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/_parsers.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/_parsers.py",
            "functionName": "summary",
            "lines": [
              {
                "startLine": 1380,
                "endLine": 1384
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/_parsers.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "summary",
            "filePath": "pretty_bad_protocol/_parsers.py",
            "uncoveredLines": [
              {
                "startLine": 1380,
                "endLine": 1384
              }
            ]
          },
          "uncoveredFnBody": "class ImportResult:\n    \"\"\"Parse GnuPG status messages for key import operations.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key import operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: A map from GnuPG codes shown with the ``IMPORT_OK`` status message\n        #: to their human-meaningful English equivalents.\n        self._ok_reason = {\n            \"0\": \"Not actually changed\",\n            \"1\": \"Entirely new key\",\n            \"2\": \"New user IDs\",\n            \"4\": \"New signatures\",\n            \"8\": \"New subkeys\",\n            \"16\": \"Contains private key\",\n            \"17\": \"Contains private key\",\n        }\n\n        #: A map from GnuPG codes shown with the ``IMPORT_PROBLEM`` status\n        #: message to their human-meaningful English equivalents.\n        self._problem_reason = {\n            \"0\": \"No specific reason given\",\n            \"1\": \"Invalid Certificate\",\n            \"2\": \"Issuer Certificate missing\",\n            \"3\": \"Certificate Chain too long\",\n            \"4\": \"Error storing certificate\",\n        }\n\n        #: All the possible status messages pertaining to actions taken while\n        #: importing a key.\n        self._fields = \"\"\"count no_user_id imported imported_rsa unchanged\n        n_uids n_subk n_sigs n_revoc sec_read sec_imported sec_dups\n        not_imported\"\"\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: imported.\n        self.fingerprints = list()\n\n        #: A list containing dictionaries with information gathered on keys\n        #: imported.\n        self.results = list()\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have imported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0\n\n    __bool__ = __nonzero__\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        if key == \"IMPORTED\":\n            # this duplicates info we already see in import_ok & import_problem\n            pass\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n        elif key == \"KEY_CONSIDERED\":\n            self.results.append(\n                {\n                    \"status\": key.replace(\"_\", \" \").lower(),\n                }\n            )\n        elif key == \"NODATA\":\n            self.results.append({\"fingerprint\": None, \"status\": \"No valid data found\"})\n        elif key == \"IMPORT_OK\":\n            reason, fingerprint = value.split()\n            reasons = []\n            for code, text in self._ok_reason.items():\n                if int(reason) == int(code):\n                    reasons.append(text)\n            reasontext = \"\\n\".join(reasons) + \"\\n\"\n            self.results.append({\"fingerprint\": fingerprint, \"status\": reasontext})\n            self.fingerprints.append(fingerprint)\n        elif key == \"IMPORT_PROBLEM\":\n            try:\n                reason, fingerprint = value.split()\n            except:  # noqa: E722\n                reason = value\n                fingerprint = \"<unknown>\"\n            self.results.append(\n                {\"fingerprint\": fingerprint, \"status\": self._problem_reason[reason]}\n            )\n        elif key == \"IMPORT_RES\":\n            import_res = value.split()\n            for x in self.counts:\n                self.counts[x] = int(import_res.pop(0))\n        elif key == \"KEYEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Key expired\"}\n            self.results.append(res)\n        # Accoring to docs/DETAILS L859, SIGEXPIRED is obsolete:\n        # \"Removed on 2011-02-04. This is deprecated in favor of KEYEXPIRED.\"\n        elif key == \"SIGEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Signature expired\"}\n            self.results.append(res)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n    def summary(self):  # type: ignore[no-untyped-def]\n        l = []  # noqa: E741 # Untested\n        l.append(\"%d imported\" % self.counts[\"imported\"]) # Untested\n        if self.counts[\"not_imported\"]: # Untested\n            l.append(\"%d not imported\" % self.counts[\"not_imported\"]) # Untested\n        return \", \".join(l) # Untested",
          "callGraphToTestedFunction": [
            "summary"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "verify",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "verify",
            "lines": [
              {
                "startLine": 258,
                "endLine": 261
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "verify",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 258,
                "endLine": 261
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding) # Untested\n        result = self.verify_file(f) # Untested\n        f.close() # Untested\n        return result # Untested\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "verify"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "recv_keys",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "recv_keys",
            "lines": [
              {
                "startLine": 359,
                "endLine": 361
              },
              {
                "startLine": 363,
                "endLine": 363
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "recv_keys",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 359,
                "endLine": 361
              },
              {
                "startLine": 363,
                "endLine": 363
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids: # Untested\n            keys = \" \".join([key for key in keyids]) # Untested\n            return self._recv_keys(keys, **kwargs) # Untested\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "recv_keys"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "delete_keys",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "delete_keys",
            "lines": [
              {
                "startLine": 391,
                "endLine": 391
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "delete_keys",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 391,
                "endLine": 391
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints) # Untested\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "delete_keys"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "list_keys",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "list_keys",
            "lines": [
              {
                "startLine": 456,
                "endLine": 456
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "list_keys",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 456,
                "endLine": 456
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\" # Untested\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "list_keys"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "list_packets",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "list_packets",
            "lines": [
              {
                "startLine": 478,
                "endLine": 481
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "list_packets",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 478,
                "endLine": 481
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"] # Untested\n        result = self._result_map[\"packets\"](self) # Untested\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result) # Untested\n        return result # Untested\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "list_packets"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "list_sigs",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "list_sigs",
            "lines": [
              {
                "startLine": 532,
                "endLine": 532
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "list_sigs",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 532,
                "endLine": 532
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids) # Untested\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "list_sigs"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "check_sigs",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "check_sigs",
            "lines": [
              {
                "startLine": 541,
                "endLine": 541
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_sigs",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 541,
                "endLine": 541
              }
            ]
          },
          "uncoveredFnBody": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True) # Untested\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value default\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are encrypt, sign, and auth. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the cert flag will be on. If no\n            Key-Usage is specified and the Key-Type is not default, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but default is used the usage will be sign.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command setpref in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            sensitive flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "callGraphToTestedFunction": [
            "check_sigs"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "find_key_by_email",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "find_key_by_email",
            "lines": [
              {
                "startLine": 1104,
                "endLine": 1108
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "find_key_by_email",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1104,
                "endLine": 1108
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret): # Untested\n            for uid in key[\"uids\"]: # Untested\n                if re.search(email, uid): # Untested\n                    return key # Untested\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email) # Untested\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "callGraphToTestedFunction": [
            "find_key_by_email"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "find_key_by_subkey",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "find_key_by_subkey",
            "lines": [
              {
                "startLine": 1115,
                "endLine": 1119
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "find_key_by_subkey",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1115,
                "endLine": 1119
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys(): # Untested\n            for sub in key[\"subkeys\"]: # Untested\n                if sub[0] == subkey: # Untested\n                    return key # Untested\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey) # Untested\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "callGraphToTestedFunction": [
            "find_key_by_subkey"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_encrypted_sym",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "is_encrypted_sym",
            "lines": [
              {
                "startLine": 1145,
                "endLine": 1146
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_encrypted_sym",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1145,
                "endLine": 1146
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data) # Untested\n        return bool(result.need_passphrase_sym) # Untested\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "callGraphToTestedFunction": [
            "is_encrypted_sym"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_encrypted_asym",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "is_encrypted_asym",
            "lines": [
              {
                "startLine": 1149,
                "endLine": 1150
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_encrypted_asym",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1149,
                "endLine": 1150
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data) # Untested\n        return bool(result.key) # Untested\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "callGraphToTestedFunction": [
            "is_encrypted_asym"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "is_encrypted",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "is_encrypted",
            "lines": [
              {
                "startLine": 1153,
                "endLine": 1153
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "is_encrypted",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1153,
                "endLine": 1153
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data) # Untested",
          "callGraphToTestedFunction": [
            "is_encrypted"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "get_current",
        "replayTestFileNames": [],
        "file": "sdconfig.py",
        "callGraph": [
          {
            "file": "sdconfig.py",
            "functionName": "get_current",
            "lines": [
              {
                "startLine": 101,
                "endLine": 101
              },
              {
                "startLine": 103,
                "endLine": 104
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "sdconfig.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "get_current",
            "filePath": "sdconfig.py",
            "uncoveredLines": [
              {
                "startLine": 101,
                "endLine": 101
              },
              {
                "startLine": 103,
                "endLine": 104
              }
            ]
          },
          "uncoveredFnBody": "@dataclass(frozen=True)\nclass SecureDropConfig:\n    JOURNALIST_APP_FLASK_CONFIG_CLS: JournalistInterfaceConfig\n    SOURCE_APP_FLASK_CONFIG_CLS: SourceInterfaceConfig\n\n    GPG_KEY_DIR: Path\n    JOURNALIST_KEY: str\n    SCRYPT_GPG_PEPPER: str\n    SCRYPT_ID_PEPPER: str\n    SCRYPT_PARAMS: Dict[str, int]\n\n    SECUREDROP_DATA_ROOT: Path\n\n    DATABASE_FILE: Path  # Path to the sqlite DB file\n\n    SECUREDROP_ROOT: Path\n    STATIC_DIR: Path\n    TRANSLATION_DIRS: Path\n    SOURCE_TEMPLATES_DIR: Path\n    JOURNALIST_TEMPLATES_DIR: Path\n    NOUNS: Path\n    ADJECTIVES: Path\n\n    DEFAULT_LOCALE: str\n    SUPPORTED_LOCALES: List[str]\n\n    SESSION_EXPIRATION_MINUTES: float\n\n    RQ_WORKER_NAME: str\n\n    REDIS_PASSWORD: str\n\n    env: str = \"prod\"\n\n    @property\n    def TEMP_DIR(self) -> Path:\n        # We use a directory under the SECUREDROP_DATA_ROOT instead of `/tmp` because\n        # we need to expose this directory via X-Send-File, and want to minimize the\n        # potential for exposing unintended files.\n        return self.SECUREDROP_DATA_ROOT / \"tmp\"\n\n    @property\n    def STORE_DIR(self) -> Path:\n        return self.SECUREDROP_DATA_ROOT / \"store\"\n\n    @property\n    def DATABASE_URI(self) -> str:\n        return f\"sqlite:///{self.DATABASE_FILE}\"\n\n    @property\n    def REDIS_KWARGS(self) -> Dict[str, str]:\n        \"\"\"kwargs to pass to `redis.Redis` constructor\"\"\"\n        return {\"password\": self.REDIS_PASSWORD}\n\n    @classmethod\n    def get_current(cls) -> \"SecureDropConfig\":\n        global _current_config\n        if _current_config is None:\n            # Retrieve the config by parsing it from ./config.py\n            _current_config = _parse_config_from_file(config_module_name=\"config\") # Untested\n        return _current_config # Untested",
          "callGraphToTestedFunction": [
            "get_current"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "safe_renames",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "safe_renames",
            "lines": [
              {
                "startLine": 78,
                "endLine": 78
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "safe_renames",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 78,
                "endLine": 78
              }
            ]
          },
          "uncoveredFnBody": "def safe_renames(old: str, new: str) -> None:\n    \"\"\"safe_renames(old, new)\n\n    This is a modified version of Python's os.renames that does not\n    prune directories.\n    Super-rename; create directories as necessary without deleting any\n    left empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n    \"\"\"\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head) # Untested\n    os.rename(old, new)",
          "callGraphToTestedFunction": [
            "safe_renames"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "shredder_path",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "shredder_path",
            "lines": [
              {
                "startLine": 115,
                "endLine": 115
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "shredder_path",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 115,
                "endLine": 115
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path # Untested\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "shredder_path"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "shredder_contains",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "shredder_contains",
            "lines": [
              {
                "startLine": 121,
                "endLine": 122
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "shredder_contains",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 121,
                "endLine": 122
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path)) # Untested\n        return common_path == self.__shredder_path # Untested\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "shredder_contains"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "verify",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "verify",
            "lines": [
              {
                "startLine": 148,
                "endLine": 148
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "verify",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 148,
                "endLine": 148
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\") # Untested\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "verify"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "move_to_shredder",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "move_to_shredder",
            "lines": [
              {
                "startLine": 247,
                "endLine": 247
              },
              {
                "startLine": 250,
                "endLine": 250
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "move_to_shredder",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 247,
                "endLine": 247
              },
              {
                "startLine": 250,
                "endLine": 250
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\") # Untested\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "move_to_shredder"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "save_file_submission",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "save_file_submission",
            "lines": [
              {
                "startLine": 308,
                "endLine": 308
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "save_file_submission",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 308,
                "endLine": 308
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\") # Untested\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "save_file_submission"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "random_base32",
        "replayTestFileNames": [],
        "file": "two_factor.py",
        "callGraph": [
          {
            "file": "two_factor.py",
            "functionName": "random_base32",
            "lines": [
              {
                "startLine": 18,
                "endLine": 18
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "two_factor.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "random_base32",
            "filePath": "two_factor.py",
            "uncoveredLines": [
              {
                "startLine": 18,
                "endLine": 18
              }
            ]
          },
          "uncoveredFnBody": "def random_base32(length: int = 32) -> str:\n    if length < 32:\n        raise ValueError(\"Secrets should be at least 160 bits\") # Untested\n\n    chars_to_use = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\")\n    return \"\".join(secrets.choice(chars_to_use) for _ in range(length))",
          "callGraphToTestedFunction": [
            "random_base32"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "format_secret",
        "replayTestFileNames": [],
        "file": "two_factor.py",
        "callGraph": [
          {
            "file": "two_factor.py",
            "functionName": "format_secret",
            "lines": [
              {
                "startLine": 28,
                "endLine": 29
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "two_factor.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "format_secret",
            "filePath": "two_factor.py",
            "uncoveredLines": [
              {
                "startLine": 28,
                "endLine": 29
              }
            ]
          },
          "uncoveredFnBody": "def format_secret(sec: str) -> str:\n    \"\"\"The OTP secret is easier to read and manually enter if it is all\n    lowercase and split into four groups of four characters. The secret is\n    base32-encoded, so it is case insensitive.\"\"\"\n    chunks = [sec[i : i + 4] for i in range(0, len(sec), 4)] # Untested\n    return \" \".join(chunks).lower() # Untested",
          "callGraphToTestedFunction": [
            "format_secret"
          ]
        }
      },
      "filteredBy": "coverage"
    },
    {
      "testTarget": {
        "functionName": "list_disconnected_fs_submissions",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "list_disconnected_fs_submissions",
            "lines": [
              {
                "startLine": 120,
                "endLine": 120
              },
              {
                "startLine": 124,
                "endLine": 127
              },
              {
                "startLine": 131,
                "endLine": 132
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "list_disconnected_fs_submissions",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 120,
                "endLine": 120
              },
              {
                "startLine": 124,
                "endLine": 127
              },
              {
                "startLine": 131,
                "endLine": 132
              }
            ]
          },
          "uncoveredFnBody": "def list_disconnected_fs_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    List files without a corresponding Submission or Reply record in the database.\n    \"\"\"\n    with app_context(): # Untested\n        disconnected_files = find_disconnected_fs_submissions(args.store_dir) # Untested\n        if disconnected_files: # Untested\n            print( # Untested\n                'Run \"manage.py delete-disconnected-fs-submissions\" to delete these files.',\n                file=sys.stderr,\n            )\n        for f in disconnected_files:\n            print(f)",
          "callGraphToTestedFunction": [
            "list_disconnected_fs_submissions"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "check_for_disconnected_fs_submissions",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "check_for_disconnected_fs_submissions",
            "lines": [
              {
                "startLine": 105,
                "endLine": 105
              },
              {
                "startLine": 109,
                "endLine": 112
              },
              {
                "startLine": 117,
                "endLine": 117
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_for_disconnected_fs_submissions",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 105,
                "endLine": 105
              },
              {
                "startLine": 109,
                "endLine": 112
              },
              {
                "startLine": 117,
                "endLine": 117
              }
            ]
          },
          "uncoveredFnBody": "def check_for_disconnected_fs_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Check for files without a corresponding Submission or Reply record in the database.\n    \"\"\"\n    with app_context(): # Untested\n        disconnected = find_disconnected_fs_submissions(args.store_dir) # Untested\n        if disconnected: # Untested\n            print( # Untested\n                \"There are files in the submission area with no corresponding records in the \"\n                'database. Run \"manage.py list-disconnected-fs-submissions\" for details.'\n            )\n        else:\n            print(\"No unexpected files were found in the store.\")",
          "callGraphToTestedFunction": [
            "check_for_disconnected_fs_submissions"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "set_name",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "set_name",
            "lines": [
              {
                "startLine": 477,
                "endLine": 482
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_name",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 477,
                "endLine": 482
              }
            ]
          },
          "uncoveredFnBody": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name: # Untested\n            self.check_name_acceptable(first_name) # Untested\n            self.first_name = first_name # Untested\n        if last_name: # Untested\n            self.check_name_acceptable(last_name) # Untested\n            self.last_name = last_name # Untested\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
          "callGraphToTestedFunction": [
            "set_name"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "set_clean_tmp_parser",
        "replayTestFileNames": [],
        "file": "manage.py",
        "callGraph": [
          {
            "file": "manage.py",
            "functionName": "set_clean_tmp_parser",
            "lines": [
              {
                "startLine": 405,
                "endLine": 406
              },
              {
                "startLine": 408,
                "endLine": 410
              },
              {
                "startLine": 418,
                "endLine": 418
              },
              {
                "startLine": 423,
                "endLine": 423
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "manage.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "set_clean_tmp_parser",
            "filePath": "manage.py",
            "uncoveredLines": [
              {
                "startLine": 405,
                "endLine": 406
              },
              {
                "startLine": 408,
                "endLine": 410
              },
              {
                "startLine": 418,
                "endLine": 418
              },
              {
                "startLine": 423,
                "endLine": 423
              }
            ]
          },
          "uncoveredFnBody": "def set_clean_tmp_parser(subps: _SubParsersAction, name: str) -> None:\n    config = SecureDropConfig.get_current()\n\n    parser = subps.add_parser(name, help=\"Cleanup the \" \"SecureDrop temp directory.\") # Untested\n    default_days = 7 # Untested\n    parser.add_argument( # Untested\n        \"--days\",\n        default=default_days,\n        type=int,\n        help=(\n            \"remove files not modified in a given number of DAYS \" f\"(default {default_days} days)\"\n        ),\n    )\n    parser.add_argument(\n        \"--directory\",\n        default=config.TEMP_DIR,\n        help=(\"remove old files from DIRECTORY \" f\"(default {config.TEMP_DIR})\"),\n    )\n    parser.set_defaults(func=clean_tmp)",
          "callGraphToTestedFunction": [
            "set_clean_tmp_parser"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "save_pre_encrypted_reply",
        "replayTestFileNames": [],
        "file": "store.py",
        "callGraph": [
          {
            "file": "store.py",
            "functionName": "save_pre_encrypted_reply",
            "lines": [
              {
                "startLine": 345,
                "endLine": 346
              },
              {
                "startLine": 348,
                "endLine": 349
              },
              {
                "startLine": 351,
                "endLine": 352
              },
              {
                "startLine": 354,
                "endLine": 354
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "store.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "save_pre_encrypted_reply",
            "filePath": "store.py",
            "uncoveredLines": [
              {
                "startLine": 345,
                "endLine": 346
              },
              {
                "startLine": 348,
                "endLine": 349
              },
              {
                "startLine": 351,
                "endLine": 352
              },
              {
                "startLine": 354,
                "endLine": 354
              }
            ]
          },
          "uncoveredFnBody": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh: # Untested\n            fh.write(content) # Untested\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
          "callGraphToTestedFunction": [
            "save_pre_encrypted_reply"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "documents_messages_count",
        "replayTestFileNames": [],
        "file": "models.py",
        "callGraph": [
          {
            "file": "models.py",
            "functionName": "documents_messages_count",
            "lines": [
              {
                "startLine": 95,
                "endLine": 101
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "models.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "documents_messages_count",
            "filePath": "models.py",
            "uncoveredLines": [
              {
                "startLine": 95,
                "endLine": 101
              }
            ]
          },
          "uncoveredFnBody": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0} # Untested\n        for submission in self.submissions: # Untested\n            if submission.is_message: # Untested\n                self.docs_msgs_count[\"messages\"] += 1 # Untested\n            elif submission.is_file: # Untested\n                self.docs_msgs_count[\"documents\"] += 1 # Untested\n        return self.docs_msgs_count # Untested\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred:\n            starred = True\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
          "callGraphToTestedFunction": [
            "documents_messages_count"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "send_keys",
        "replayTestFileNames": [],
        "file": "pretty_bad_protocol/gnupg.py",
        "callGraph": [
          {
            "file": "pretty_bad_protocol/gnupg.py",
            "functionName": "send_keys",
            "lines": [
              {
                "startLine": 1123,
                "endLine": 1131
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "pretty_bad_protocol/gnupg.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "send_keys",
            "filePath": "pretty_bad_protocol/gnupg.py",
            "uncoveredLines": [
              {
                "startLine": 1123,
                "endLine": 1131
              }
            ]
          },
          "uncoveredFnBody": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self) # Untested\n        log.debug(\"send_keys: %r\", keyids) # Untested\n        data = _util._make_binary_stream(\"\", self._encoding) # Untested\n        args = [\"--keyserver\", keyserver, \"--send-keys\"] # Untested\n        args.extend(keyids) # Untested\n        self._handle_io(args, data, result, binary=True) # Untested\n        log.debug(\"send_keys result: %r\", result.__dict__) # Untested\n        data.close() # Untested\n        return result # Untested\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "callGraphToTestedFunction": [
            "send_keys"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "list_disconnected_db_submissions",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "list_disconnected_db_submissions",
            "lines": [
              {
                "startLine": 45,
                "endLine": 45
              },
              {
                "startLine": 49,
                "endLine": 52
              },
              {
                "startLine": 56,
                "endLine": 57
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "list_disconnected_db_submissions",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 45,
                "endLine": 45
              },
              {
                "startLine": 49,
                "endLine": 52
              },
              {
                "startLine": 56,
                "endLine": 57
              }
            ]
          },
          "uncoveredFnBody": "def list_disconnected_db_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    List the IDs of Submission records whose files are missing.\n    \"\"\"\n    with app_context(): # Untested\n        disconnected_submissions = find_disconnected_db_submissions(args.store_dir) # Untested\n        if disconnected_submissions: # Untested\n            print( # Untested\n                'Run \"manage.py delete-disconnected-db-submissions\" to delete these records.',\n                file=sys.stderr,\n            )\n        for s in disconnected_submissions:\n            print(s.id)",
          "callGraphToTestedFunction": [
            "list_disconnected_db_submissions"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "check_for_disconnected_db_submissions",
        "replayTestFileNames": [],
        "file": "management/submissions.py",
        "callGraph": [
          {
            "file": "management/submissions.py",
            "functionName": "check_for_disconnected_db_submissions",
            "lines": [
              {
                "startLine": 30,
                "endLine": 30
              },
              {
                "startLine": 34,
                "endLine": 37
              },
              {
                "startLine": 42,
                "endLine": 42
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "management/submissions.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "check_for_disconnected_db_submissions",
            "filePath": "management/submissions.py",
            "uncoveredLines": [
              {
                "startLine": 30,
                "endLine": 30
              },
              {
                "startLine": 34,
                "endLine": 37
              },
              {
                "startLine": 42,
                "endLine": 42
              }
            ]
          },
          "uncoveredFnBody": "def check_for_disconnected_db_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Check for Submission records whose files are missing.\n    \"\"\"\n    with app_context(): # Untested\n        disconnected = find_disconnected_db_submissions(args.store_dir) # Untested\n        if disconnected: # Untested\n            print( # Untested\n                \"There are submissions in the database with no corresponding files. \"\n                'Run \"manage.py list-disconnected-db-submissions\" for details.'\n            )\n        else:\n            print(\"No problems were found. All submissions' files are present.\")",
          "callGraphToTestedFunction": [
            "check_for_disconnected_db_submissions"
          ]
        }
      },
      "filteredBy": "importance"
    },
    {
      "testTarget": {
        "functionName": "qrcode_svg",
        "replayTestFileNames": [],
        "file": "two_factor.py",
        "callGraph": [
          {
            "file": "two_factor.py",
            "functionName": "qrcode_svg",
            "lines": [
              {
                "startLine": 153,
                "endLine": 153
              },
              {
                "startLine": 155,
                "endLine": 157
              },
              {
                "startLine": 159,
                "endLine": 161
              }
            ],
            "replayTestFileNames": []
          }
        ],
        "replayTestFilePaths": [],
        "filePath": "two_factor.py",
        "uncoveredTarget": {
          "uncoveredFile": {
            "functionName": "qrcode_svg",
            "filePath": "two_factor.py",
            "uncoveredLines": [
              {
                "startLine": 153,
                "endLine": 153
              },
              {
                "startLine": 155,
                "endLine": 157
              },
              {
                "startLine": 159,
                "endLine": 161
              }
            ]
          },
          "uncoveredFnBody": "class TOTP:\n    # Current parameters for TOTP\n    _LENGTH = 6\n    _TIME_STEP = 30\n\n    # nosemgrep: python.cryptography.security.insecure-hash-algorithms.insecure-hash-algorithm-sha1\n    _ALGORITHM = SHA1()  # noqa: S303\n\n    # Minimum length for ascii-encoded OTP secrets - by default, secrets are now 160-bit (32 chars)\n    # but existing Journalist users may still have 80-bit (16-char) secrets\n    _SECRET_MIN_BASE32_LENGTH = 16  # 80 bits == 40 hex digits (== 16 ascii-encoded chars in db)\n\n    def __init__(self, secret_as_base32: str) -> None:\n        if len(secret_as_base32) < self._SECRET_MIN_BASE32_LENGTH:\n            raise OtpSecretInvalid(\"Invalid secret length\")\n\n        try:\n            otp_secret_as_bytes = base64.b32decode(\n                # Need casefold=True because the base32 secret we receive from the UI might be\n                # lowercase\n                secret_as_base32,\n                casefold=True,\n            )\n        except binascii.Error:\n            raise OtpSecretInvalid(\"Secret is not base32-encoded\")\n\n        self._totp = totp.TOTP(\n            key=otp_secret_as_bytes,\n            length=self._LENGTH,\n            algorithm=self._ALGORITHM,\n            time_step=self._TIME_STEP,\n            # Existing Journalist users may still have 80-bit (16-char) secrets\n            enforce_key_length=False,\n        )\n\n    def generate(self, time: datetime) -> str:\n        return self._totp.generate(time.timestamp()).decode(\"ascii\")\n\n    def now(self) -> str:\n        return self._totp.generate(datetime.utcnow().timestamp()).decode(\"ascii\")\n\n    def verify(self, token: str, time: datetime) -> None:\n        # Also check the given token against the previous and next valid tokens, to compensate\n        # for potential time skew between the client and the server. The total valid window is 1:30s\n        token_verification_succeeded = False\n        for index_for_time_skew in [-1, 0, 1]:\n            time_for_time_skew = int(time.timestamp()) + self._TIME_STEP * index_for_time_skew\n            try:\n                self._totp.verify(token.encode(\"ascii\"), time_for_time_skew)\n                token_verification_succeeded = True\n                break\n            except InvalidToken:\n                pass\n\n        if not token_verification_succeeded:\n            raise OtpTokenInvalid(\"Token verification failed\")\n\n    def get_provisioning_uri(self, account_name: str) -> str:\n        return self._totp.get_provisioning_uri(account_name=account_name, issuer=\"SecureDrop\")\n\n    def qrcode_svg(self, account_name: str) -> bytes:\n        uri = self.get_provisioning_uri(account_name)\n\n        qr = qrcode.QRCode(box_size=15, image_factory=qrcode.image.svg.SvgPathImage)\n        qr.add_data(uri)\n        img = qr.make_image()\n\n        svg_out = BytesIO() # Untested\n        img.save(svg_out) # Untested\n        return svg_out.getvalue() # Untested",
          "callGraphToTestedFunction": [
            "qrcode_svg"
          ]
        }
      },
      "filteredBy": "importance"
    }
  ]
}
